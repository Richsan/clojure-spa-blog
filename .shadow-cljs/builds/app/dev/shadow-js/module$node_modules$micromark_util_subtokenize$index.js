["^ ","~:resource-id",["~:shadow.build.npm/resource","node_modules/micromark-util-subtokenize/index.js"],"~:js","shadow$provide.module$node_modules$micromark_util_subtokenize$index=function(global,require,module,exports){Object.defineProperty(exports,\"__esModule\",{value:!0});exports.subtokenize=function(events){const jumps={};let index$jscomp$0=-1;for(var event,lineIndex,otherIndex,otherEvent,more;++index$jscomp$0<events.length;){for(;index$jscomp$0 in jumps;)index$jscomp$0=jumps[index$jscomp$0];event=events[index$jscomp$0];if(index$jscomp$0&&\"chunkFlow\"===event[1].type&&\"listItemPrefix\"===events[index$jscomp$0-\n1][1].type&&(lineIndex=event[1]._tokenizer.events,otherIndex=0,otherIndex<lineIndex.length&&\"lineEndingBlank\"===lineIndex[otherIndex][1].type&&(otherIndex+=2),otherIndex<lineIndex.length&&\"content\"===lineIndex[otherIndex][1].type))for(;++otherIndex<lineIndex.length&&\"content\"!==lineIndex[otherIndex][1].type;)\"chunkText\"===lineIndex[otherIndex][1].type&&(lineIndex[otherIndex][1]._isInFirstContentOfListItem=!0,otherIndex++);if(\"enter\"===event[0]){if(event[1].contentType){event=Object;more=event.assign;\nvar start=void 0;start=void 0;let stream;lineIndex=events;const token=lineIndex[index$jscomp$0][1],context=lineIndex[index$jscomp$0][2];let startPosition=index$jscomp$0-1;otherIndex=[];var tokenizer=token._tokenizer||context.parser[token.contentType](token.start);otherEvent=tokenizer.events;const jumps$jscomp$0=[],gaps={};let index=-1;var current=token;let adjust=0;const breaks=[0];for(;current;){for(;lineIndex[++startPosition][1]!==current;);otherIndex.push(startPosition);current._tokenizer||(stream=\ncontext.sliceStream(current),current.next||stream.push(null),start&&tokenizer.defineSkip(current.start),current._isInFirstContentOfListItem&&(tokenizer._gfmTasklistFirstContentOfListItem=!0),tokenizer.write(stream),current._isInFirstContentOfListItem&&(tokenizer._gfmTasklistFirstContentOfListItem=void 0));start=current;current=current.next}for(current=token;++index<otherEvent.length;)\"exit\"===otherEvent[index][0]&&\"enter\"===otherEvent[index-1][0]&&otherEvent[index][1].type===otherEvent[index-1][1].type&&\notherEvent[index][1].start.line!==otherEvent[index][1].end.line&&(start=index+1,breaks.push(start),current._tokenizer=void 0,current.previous=void 0,current=current.next);tokenizer.events=[];current?(current._tokenizer=void 0,current.previous=void 0):breaks.pop();for(index=breaks.length;index--;)tokenizer=otherEvent.slice(breaks[index],breaks[index+1]),current=otherIndex.pop(),jumps$jscomp$0.unshift([current,current+tokenizer.length-1]),(0,_micromarkUtilChunked.splice)(lineIndex,current,2,tokenizer);\nfor(index=-1;++index<jumps$jscomp$0.length;)gaps[adjust+jumps$jscomp$0[index][0]]=adjust+jumps$jscomp$0[index][1],adjust+=jumps$jscomp$0[index][1]-jumps$jscomp$0[index][0]-1;more.call(event,jumps,gaps);index$jscomp$0=jumps[index$jscomp$0];more=!0}}else if(event[1]._container){otherIndex=index$jscomp$0;for(lineIndex=void 0;otherIndex--;)if(otherEvent=events[otherIndex],\"lineEnding\"===otherEvent[1].type||\"lineEndingBlank\"===otherEvent[1].type)\"enter\"===otherEvent[0]&&(lineIndex&&(events[lineIndex][1].type=\n\"lineEndingBlank\"),otherEvent[1].type=\"lineEnding\",lineIndex=otherIndex);else break;lineIndex&&(event[1].end=Object.assign({},events[lineIndex][1].start),otherIndex=events.slice(lineIndex,index$jscomp$0),otherIndex.unshift(event),(0,_micromarkUtilChunked.splice)(events,lineIndex,index$jscomp$0-lineIndex+1,otherIndex))}}return!more};var _micromarkUtilChunked=require(\"module$node_modules$micromark_util_chunked$index\")}","~:source","shadow$provide[\"module$node_modules$micromark_util_subtokenize$index\"] = function(global,require,module,exports) {\n\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.subtokenize = subtokenize;\n\nvar _micromarkUtilChunked = require(\"micromark-util-chunked\");\n\n/**\n * @typedef {import('micromark-util-types').Token} Token\n * @typedef {import('micromark-util-types').Chunk} Chunk\n * @typedef {import('micromark-util-types').Event} Event\n */\n\n/**\n * Tokenize subcontent.\n *\n * @param {Event[]} events\n * @returns {boolean}\n */\nfunction subtokenize(events) {\n  /** @type {Record<string, number>} */\n  const jumps = {};\n  let index = -1;\n  /** @type {Event} */\n\n  let event;\n  /** @type {number|undefined} */\n\n  let lineIndex;\n  /** @type {number} */\n\n  let otherIndex;\n  /** @type {Event} */\n\n  let otherEvent;\n  /** @type {Event[]} */\n\n  let parameters;\n  /** @type {Event[]} */\n\n  let subevents;\n  /** @type {boolean|undefined} */\n\n  let more;\n\n  while (++index < events.length) {\n    while (index in jumps) {\n      index = jumps[index];\n    }\n\n    event = events[index]; // Add a hook for the GFM tasklist extension, which needs to know if text\n    // is in the first content of a list item.\n\n    if (index && event[1].type === 'chunkFlow' && events[index - 1][1].type === 'listItemPrefix') {\n      subevents = event[1]._tokenizer.events;\n      otherIndex = 0;\n\n      if (otherIndex < subevents.length && subevents[otherIndex][1].type === 'lineEndingBlank') {\n        otherIndex += 2;\n      }\n\n      if (otherIndex < subevents.length && subevents[otherIndex][1].type === 'content') {\n        while (++otherIndex < subevents.length) {\n          if (subevents[otherIndex][1].type === 'content') {\n            break;\n          }\n\n          if (subevents[otherIndex][1].type === 'chunkText') {\n            subevents[otherIndex][1]._isInFirstContentOfListItem = true;\n            otherIndex++;\n          }\n        }\n      }\n    } // Enter.\n\n\n    if (event[0] === 'enter') {\n      if (event[1].contentType) {\n        Object.assign(jumps, subcontent(events, index));\n        index = jumps[index];\n        more = true;\n      }\n    } // Exit.\n    else if (event[1]._container) {\n      otherIndex = index;\n      lineIndex = undefined;\n\n      while (otherIndex--) {\n        otherEvent = events[otherIndex];\n\n        if (otherEvent[1].type === 'lineEnding' || otherEvent[1].type === 'lineEndingBlank') {\n          if (otherEvent[0] === 'enter') {\n            if (lineIndex) {\n              events[lineIndex][1].type = 'lineEndingBlank';\n            }\n\n            otherEvent[1].type = 'lineEnding';\n            lineIndex = otherIndex;\n          }\n        } else {\n          break;\n        }\n      }\n\n      if (lineIndex) {\n        // Fix position.\n        event[1].end = Object.assign({}, events[lineIndex][1].start); // Switch container exit w/ line endings.\n\n        parameters = events.slice(lineIndex, index);\n        parameters.unshift(event);\n        (0, _micromarkUtilChunked.splice)(events, lineIndex, index - lineIndex + 1, parameters);\n      }\n    }\n  }\n\n  return !more;\n}\n/**\n * Tokenize embedded tokens.\n *\n * @param {Event[]} events\n * @param {number} eventIndex\n * @returns {Record<string, number>}\n */\n\n\nfunction subcontent(events, eventIndex) {\n  const token = events[eventIndex][1];\n  const context = events[eventIndex][2];\n  let startPosition = eventIndex - 1;\n  /** @type {number[]} */\n\n  const startPositions = [];\n  const tokenizer = token._tokenizer || context.parser[token.contentType](token.start);\n  const childEvents = tokenizer.events;\n  /** @type {[number, number][]} */\n\n  const jumps = [];\n  /** @type {Record<string, number>} */\n\n  const gaps = {};\n  /** @type {Chunk[]} */\n\n  let stream;\n  /** @type {Token|undefined} */\n\n  let previous;\n  let index = -1;\n  /** @type {Token|undefined} */\n\n  let current = token;\n  let adjust = 0;\n  let start = 0;\n  const breaks = [start]; // Loop forward through the linked tokens to pass them in order to the\n  // subtokenizer.\n\n  while (current) {\n    // Find the position of the event for this token.\n    while (events[++startPosition][1] !== current) {// Empty.\n    }\n\n    startPositions.push(startPosition);\n\n    if (!current._tokenizer) {\n      stream = context.sliceStream(current);\n\n      if (!current.next) {\n        stream.push(null);\n      }\n\n      if (previous) {\n        tokenizer.defineSkip(current.start);\n      }\n\n      if (current._isInFirstContentOfListItem) {\n        tokenizer._gfmTasklistFirstContentOfListItem = true;\n      }\n\n      tokenizer.write(stream);\n\n      if (current._isInFirstContentOfListItem) {\n        tokenizer._gfmTasklistFirstContentOfListItem = undefined;\n      }\n    } // Unravel the next token.\n\n\n    previous = current;\n    current = current.next;\n  } // Now, loop back through all events (and linked tokens), to figure out which\n  // parts belong where.\n\n\n  current = token;\n\n  while (++index < childEvents.length) {\n    if ( // Find a void token that includes a break.\n    childEvents[index][0] === 'exit' && childEvents[index - 1][0] === 'enter' && childEvents[index][1].type === childEvents[index - 1][1].type && childEvents[index][1].start.line !== childEvents[index][1].end.line) {\n      start = index + 1;\n      breaks.push(start); // Help GC.\n\n      current._tokenizer = undefined;\n      current.previous = undefined;\n      current = current.next;\n    }\n  } // Help GC.\n\n\n  tokenizer.events = []; // If there’s one more token (which is the cases for lines that end in an\n  // EOF), that’s perfect: the last point we found starts it.\n  // If there isn’t then make sure any remaining content is added to it.\n\n  if (current) {\n    // Help GC.\n    current._tokenizer = undefined;\n    current.previous = undefined;\n  } else {\n    breaks.pop();\n  } // Now splice the events from the subtokenizer into the current events,\n  // moving back to front so that splice indices aren’t affected.\n\n\n  index = breaks.length;\n\n  while (index--) {\n    const slice = childEvents.slice(breaks[index], breaks[index + 1]);\n    const start = startPositions.pop();\n    jumps.unshift([start, start + slice.length - 1]);\n    (0, _micromarkUtilChunked.splice)(events, start, 2, slice);\n  }\n\n  index = -1;\n\n  while (++index < jumps.length) {\n    gaps[adjust + jumps[index][0]] = adjust + jumps[index][1];\n    adjust += jumps[index][1] - jumps[index][0] - 1;\n  }\n\n  return gaps;\n}\n};","~:removed-requires",["~#set",[]],"~:actual-requires",["^5",["~$shadow.js","~$module$node_modules$micromark_util_chunked$index"]],"~:properties",["^5",["__esModule","_gfmTasklistFirstContentOfListItem","value","previous","subtokenize","events","_isInFirstContentOfListItem","_tokenizer","type","end"]],"~:compiled-at",1676665867283,"~:source-map-json","{\n\"version\":3,\n\"file\":\"module$node_modules$micromark_util_subtokenize$index.js\",\n\"lineCount\":7,\n\"mappings\":\"AAAAA,cAAA,CAAA,oDAAA,CAAyE,QAAQ,CAACC,MAAD,CAAQC,OAAR,CAAgBC,MAAhB,CAAuBC,OAAvB,CAAgC,CAGjHC,MAAOC,CAAAA,cAAP,CAAsBF,OAAtB,CAA+B,YAA/B,CAA6C,CAC3CG,MAAO,CAAA,CADoC,CAA7C,CAGAH,QAAQI,CAAAA,WAAR,CAgBAA,QAAoB,CAACC,MAAD,CAAS,CAE3B,MAAMC,MAAQ,EACd,KAAIC,eAAQ,CAAC,CAuBb,KApBA,IAAIC,KAAJ,CAGIC,SAHJ,CAMIC,UANJ,CASIC,UATJ,CAkBIC,IAEJ,CAAO,EAAEL,cAAT,CAAiBF,MAAOQ,CAAAA,MAAxB,CAAA,CAAgC,CAC9B,IAAA,CAAON,cAAP,GAAgBD,MAAhB,CAAA,CACEC,cAAA,CAAQD,KAAA,CAAMC,cAAN,CAGVC,MAAA,CAAQH,MAAA,CAAOE,cAAP,CAGR,IAAIA,cAAJ,EAA+B,WAA/B,GAAaC,KAAA,CAAM,CAAN,CAASM,CAAAA,IAAtB,EAA4E,gBAA5E,GAA8CT,MAAA,CAAOE,cAAP;AAAe,CAAf,CAAA,CAAkB,CAAlB,CAAqBO,CAAAA,IAAnE,GACEC,SAOI,CAPQP,KAAA,CAAM,CAAN,CAASQ,CAAAA,UAAWX,CAAAA,MAO5B,CANJK,UAMI,CANS,CAMT,CAJAA,UAIA,CAJaK,SAAUF,CAAAA,MAIvB,EAJmE,iBAInE,GAJiCE,SAAA,CAAUL,UAAV,CAAA,CAAsB,CAAtB,CAAyBI,CAAAA,IAI1D,GAHFJ,UAGE,EAHY,CAGZ,EAAAA,UAAA,CAAaK,SAAUF,CAAAA,MAAvB,EAAmE,SAAnE,GAAiCE,SAAA,CAAUL,UAAV,CAAA,CAAsB,CAAtB,CAAyBI,CAAAA,IARhE,EASI,IAAA,CAAO,EAAEJ,UAAT,CAAsBK,SAAUF,CAAAA,MAAhC,EACwC,SADxC,GACME,SAAA,CAAUL,UAAV,CAAA,CAAsB,CAAtB,CAAyBI,CAAAA,IAD/B,CAAA,CAKwC,WAAtC,GAAIC,SAAA,CAAUL,UAAV,CAAA,CAAsB,CAAtB,CAAyBI,CAAAA,IAA7B,GACEC,SAAA,CAAUL,UAAV,CAAA,CAAsB,CAAtB,CAAyBO,CAAAA,2BACzB,CADuD,CAAA,CACvD,CAAAP,UAAA,EAFF,CASN,IAAiB,OAAjB,GAAIF,KAAA,CAAM,CAAN,CAAJ,CACE,IAAIA,KAAA,CAAM,CAAN,CAASU,CAAAA,WAAb,CAA0B,CACxBjB,KAAAA,CAAAA,MAAOkB,KAAAA,CAAPlB,KAAOkB,CAAAA,MA0Eb;IAAIC,MAAAA,IAAAA,EANAC,MAAAA,CAAAA,IAAAA,EAHJ,KAAIC,MAjEkCjB,UAAAA,CAAAA,MAiDtC,OAAMkB,MAAQlB,SAAA,CAjDgCE,cAiDhC,CAAA,CAAmB,CAAnB,CAAd,CACMiB,QAAUnB,SAAA,CAlD8BE,cAkD9B,CAAA,CAAmB,CAAnB,CAChB,KAAIkB,cAnD0ClB,cAmD1CkB,CAA6B,CAG3BC,WAAAA,CAAiB,EACvB,KAAMC,UAAYJ,KAAMP,CAAAA,UAAlBW,EAAgCH,OAAQI,CAAAA,MAAR,CAAeL,KAAML,CAAAA,WAArB,CAAA,CAAkCK,KAAMH,CAAAA,KAAxC,CAChCS,WAAAA,CAAcF,SAAUtB,CAAAA,MAG9B,OAAMC,eAAQ,EAAd,CAGMwB,KAAO,EAOb,KAAIvB,MAAQ,CAAC,CAGb,KAAIwB,QAAUR,KACd,KAAIS,OAAS,CAEb,OAAMC,OAAS,CADHb,CACG,CAGf,KAAA,CAAOW,OAAP,CAAA,CAAgB,CAEd,IAAA,CAAO1B,SAAA,CAAO,EAAEoB,aAAT,CAAA,CAAwB,CAAxB,CAAP,GAAsCM,OAAtC,CAAA,EAGAL,UAAeQ,CAAAA,IAAf,CAAoBT,aAApB,CAEKM,QAAQf,CAAAA,UAAb,GACEM,MAgBA;AAhBSE,OAAQW,CAAAA,WAAR,CAAoBJ,OAApB,CAgBT,CAdKA,OAAQK,CAAAA,IAcb,EAbEd,MAAOY,CAAAA,IAAP,CAAY,IAAZ,CAaF,CAVIb,KAUJ,EATEM,SAAUU,CAAAA,UAAV,CAAqBN,OAAQX,CAAAA,KAA7B,CASF,CANIW,OAAQd,CAAAA,2BAMZ,GALEU,SAAUW,CAAAA,kCAKZ,CALiD,CAAA,CAKjD,EAFAX,SAAUY,CAAAA,KAAV,CAAgBjB,MAAhB,CAEA,CAAIS,OAAQd,CAAAA,2BAAZ,GACEU,SAAUW,CAAAA,kCADZ,CACiDE,IAAAA,EADjD,CAjBF,CAuBAnB,MAAA,CAAWU,OACXA,QAAA,CAAUA,OAAQK,CAAAA,IA/BJ,CAsChB,IAFAL,OAEA,CAFUR,KAEV,CAAO,EAAEhB,KAAT,CAAiBsB,UAAYhB,CAAAA,MAA7B,CAAA,CAE4B,MAD1B,GACAgB,UAAA,CAAYtB,KAAZ,CAAA,CAAmB,CAAnB,CADA,EACkE,OADlE,GACoCsB,UAAA,CAAYtB,KAAZ,CAAoB,CAApB,CAAA,CAAuB,CAAvB,CADpC,EAC6EsB,UAAA,CAAYtB,KAAZ,CAAA,CAAmB,CAAnB,CAAsBO,CAAAA,IADnG,GAC4Ge,UAAA,CAAYtB,KAAZ,CAAoB,CAApB,CAAA,CAAuB,CAAvB,CAA0BO,CAAAA,IADtI;AAC8Ie,UAAA,CAAYtB,KAAZ,CAAA,CAAmB,CAAnB,CAAsBa,CAAAA,KAAMqB,CAAAA,IAD1K,GACmLZ,UAAA,CAAYtB,KAAZ,CAAA,CAAmB,CAAnB,CAAsBmC,CAAAA,GAAID,CAAAA,IAD7M,GAEErB,KAKA,CALQb,KAKR,CALgB,CAKhB,CAJA0B,MAAOC,CAAAA,IAAP,CAAYd,KAAZ,CAIA,CAFAW,OAAQf,CAAAA,UAER,CAFqBwB,IAAAA,EAErB,CADAT,OAAQV,CAAAA,QACR,CADmBmB,IAAAA,EACnB,CAAAT,OAAA,CAAUA,OAAQK,CAAAA,IAPpB,CAYFT,UAAUtB,CAAAA,MAAV,CAAmB,EAIf0B,QAAJ,EAEEA,OAAQf,CAAAA,UACR,CADqBwB,IAAAA,EACrB,CAAAT,OAAQV,CAAAA,QAAR,CAAmBmB,IAAAA,EAHrB,EAKEP,MAAOU,CAAAA,GAAP,EAOF,KAFApC,KAEA,CAFQ0B,MAAOpB,CAAAA,MAEf,CAAON,KAAA,EAAP,CAAA,CACQqC,SAGN,CAHcf,UAAYe,CAAAA,KAAZ,CAAkBX,MAAA,CAAO1B,KAAP,CAAlB,CAAiC0B,MAAA,CAAO1B,KAAP,CAAe,CAAf,CAAjC,CAGd,CAFMa,OAEN,CAFcM,UAAeiB,CAAAA,GAAf,EAEd,CADArC,cAAMuC,CAAAA,OAAN,CAAc,CAACzB,OAAD,CAAQA,OAAR,CAAgBwB,SAAM/B,CAAAA,MAAtB,CAA+B,CAA/B,CAAd,CACA,CAAA,GAAIiC,qBAAsBC,CAAAA,MAA1B,EAAkC1C,SAAlC,CAA0Ce,OAA1C,CAAiD,CAAjD,CAAoDwB,SAApD,CAKF;IAFArC,KAEA,CAFQ,CAAC,CAET,CAAO,EAAEA,KAAT,CAAiBD,cAAMO,CAAAA,MAAvB,CAAA,CACEiB,IAAA,CAAKE,MAAL,CAAc1B,cAAA,CAAMC,KAAN,CAAA,CAAa,CAAb,CAAd,CACA,CADiCyB,MACjC,CAD0C1B,cAAA,CAAMC,KAAN,CAAA,CAAa,CAAb,CAC1C,CAAAyB,MAAA,EAAU1B,cAAA,CAAMC,KAAN,CAAA,CAAa,CAAb,CAAV,CAA4BD,cAAA,CAAMC,KAAN,CAAA,CAAa,CAAb,CAA5B,CAA8C,CA5JnCY,KAAP,CAAA,IAAA,CAAAlB,KAAA,CAAcK,KAAd,CA+JCwB,IA/JD,CACAvB,eAAA,CAAQD,KAAA,CAAMC,cAAN,CACRK,KAAA,CAAO,CAAA,CAHiB,CAA1B,CADF,IAOK,IAAIJ,KAAA,CAAM,CAAN,CAASwC,CAAAA,UAAb,CAAyB,CAC5BtC,UAAA,CAAaH,cAGb,KAFAE,SAEA,CAFY+B,IAAAA,EAEZ,CAAO9B,UAAA,EAAP,CAAA,CAGE,GAFAC,UAEI,CAFSN,MAAA,CAAOK,UAAP,CAET,CAAuB,YAAvB,GAAAC,UAAA,CAAW,CAAX,CAAcG,CAAAA,IAAd,EAA8D,iBAA9D,GAAuCH,UAAA,CAAW,CAAX,CAAcG,CAAAA,IAAzD,CACwB,OAAtB,GAAIH,UAAA,CAAW,CAAX,CAAJ,GACMF,SAKJ,GAJEJ,MAAA,CAAOI,SAAP,CAAA,CAAkB,CAAlB,CAAqBK,CAAAA,IAIvB;AAJ8B,iBAI9B,EADAH,UAAA,CAAW,CAAX,CAAcG,CAAAA,IACd,CADqB,YACrB,CAAAL,SAAA,CAAYC,UANd,CADF,KAUE,MAIAD,UAAJ,GAEED,KAAA,CAAM,CAAN,CAASkC,CAAAA,GAIT,CAJezC,MAAOkB,CAAAA,MAAP,CAAc,EAAd,CAAkBd,MAAA,CAAOI,SAAP,CAAA,CAAkB,CAAlB,CAAqBW,CAAAA,KAAvC,CAIf,CAFA6B,UAEA,CAFa5C,MAAOuC,CAAAA,KAAP,CAAanC,SAAb,CAAwBF,cAAxB,CAEb,CADA0C,UAAWJ,CAAAA,OAAX,CAAmBrC,KAAnB,CACA,CAAA,GAAIsC,qBAAsBC,CAAAA,MAA1B,EAAkC1C,MAAlC,CAA0CI,SAA1C,CAAqDF,cAArD,CAA6DE,SAA7D,CAAyE,CAAzE,CAA4EwC,UAA5E,CANF,CArB4B,CAtCA,CAsEhC,MAAO,CAACrC,IAhGmB,CAd7B,KAAIkC,sBAAwBhD,OAAA,CAAQ,kDAAR,CARqF;\",\n\"sources\":[\"node_modules/micromark-util-subtokenize/index.js\"],\n\"sourcesContent\":[\"shadow$provide[\\\"module$node_modules$micromark_util_subtokenize$index\\\"] = function(global,require,module,exports) {\\n\\\"use strict\\\";\\n\\nObject.defineProperty(exports, \\\"__esModule\\\", {\\n  value: true\\n});\\nexports.subtokenize = subtokenize;\\n\\nvar _micromarkUtilChunked = require(\\\"micromark-util-chunked\\\");\\n\\n/**\\n * @typedef {import('micromark-util-types').Token} Token\\n * @typedef {import('micromark-util-types').Chunk} Chunk\\n * @typedef {import('micromark-util-types').Event} Event\\n */\\n\\n/**\\n * Tokenize subcontent.\\n *\\n * @param {Event[]} events\\n * @returns {boolean}\\n */\\nfunction subtokenize(events) {\\n  /** @type {Record<string, number>} */\\n  const jumps = {};\\n  let index = -1;\\n  /** @type {Event} */\\n\\n  let event;\\n  /** @type {number|undefined} */\\n\\n  let lineIndex;\\n  /** @type {number} */\\n\\n  let otherIndex;\\n  /** @type {Event} */\\n\\n  let otherEvent;\\n  /** @type {Event[]} */\\n\\n  let parameters;\\n  /** @type {Event[]} */\\n\\n  let subevents;\\n  /** @type {boolean|undefined} */\\n\\n  let more;\\n\\n  while (++index < events.length) {\\n    while (index in jumps) {\\n      index = jumps[index];\\n    }\\n\\n    event = events[index]; // Add a hook for the GFM tasklist extension, which needs to know if text\\n    // is in the first content of a list item.\\n\\n    if (index && event[1].type === 'chunkFlow' && events[index - 1][1].type === 'listItemPrefix') {\\n      subevents = event[1]._tokenizer.events;\\n      otherIndex = 0;\\n\\n      if (otherIndex < subevents.length && subevents[otherIndex][1].type === 'lineEndingBlank') {\\n        otherIndex += 2;\\n      }\\n\\n      if (otherIndex < subevents.length && subevents[otherIndex][1].type === 'content') {\\n        while (++otherIndex < subevents.length) {\\n          if (subevents[otherIndex][1].type === 'content') {\\n            break;\\n          }\\n\\n          if (subevents[otherIndex][1].type === 'chunkText') {\\n            subevents[otherIndex][1]._isInFirstContentOfListItem = true;\\n            otherIndex++;\\n          }\\n        }\\n      }\\n    } // Enter.\\n\\n\\n    if (event[0] === 'enter') {\\n      if (event[1].contentType) {\\n        Object.assign(jumps, subcontent(events, index));\\n        index = jumps[index];\\n        more = true;\\n      }\\n    } // Exit.\\n    else if (event[1]._container) {\\n      otherIndex = index;\\n      lineIndex = undefined;\\n\\n      while (otherIndex--) {\\n        otherEvent = events[otherIndex];\\n\\n        if (otherEvent[1].type === 'lineEnding' || otherEvent[1].type === 'lineEndingBlank') {\\n          if (otherEvent[0] === 'enter') {\\n            if (lineIndex) {\\n              events[lineIndex][1].type = 'lineEndingBlank';\\n            }\\n\\n            otherEvent[1].type = 'lineEnding';\\n            lineIndex = otherIndex;\\n          }\\n        } else {\\n          break;\\n        }\\n      }\\n\\n      if (lineIndex) {\\n        // Fix position.\\n        event[1].end = Object.assign({}, events[lineIndex][1].start); // Switch container exit w/ line endings.\\n\\n        parameters = events.slice(lineIndex, index);\\n        parameters.unshift(event);\\n        (0, _micromarkUtilChunked.splice)(events, lineIndex, index - lineIndex + 1, parameters);\\n      }\\n    }\\n  }\\n\\n  return !more;\\n}\\n/**\\n * Tokenize embedded tokens.\\n *\\n * @param {Event[]} events\\n * @param {number} eventIndex\\n * @returns {Record<string, number>}\\n */\\n\\n\\nfunction subcontent(events, eventIndex) {\\n  const token = events[eventIndex][1];\\n  const context = events[eventIndex][2];\\n  let startPosition = eventIndex - 1;\\n  /** @type {number[]} */\\n\\n  const startPositions = [];\\n  const tokenizer = token._tokenizer || context.parser[token.contentType](token.start);\\n  const childEvents = tokenizer.events;\\n  /** @type {[number, number][]} */\\n\\n  const jumps = [];\\n  /** @type {Record<string, number>} */\\n\\n  const gaps = {};\\n  /** @type {Chunk[]} */\\n\\n  let stream;\\n  /** @type {Token|undefined} */\\n\\n  let previous;\\n  let index = -1;\\n  /** @type {Token|undefined} */\\n\\n  let current = token;\\n  let adjust = 0;\\n  let start = 0;\\n  const breaks = [start]; // Loop forward through the linked tokens to pass them in order to the\\n  // subtokenizer.\\n\\n  while (current) {\\n    // Find the position of the event for this token.\\n    while (events[++startPosition][1] !== current) {// Empty.\\n    }\\n\\n    startPositions.push(startPosition);\\n\\n    if (!current._tokenizer) {\\n      stream = context.sliceStream(current);\\n\\n      if (!current.next) {\\n        stream.push(null);\\n      }\\n\\n      if (previous) {\\n        tokenizer.defineSkip(current.start);\\n      }\\n\\n      if (current._isInFirstContentOfListItem) {\\n        tokenizer._gfmTasklistFirstContentOfListItem = true;\\n      }\\n\\n      tokenizer.write(stream);\\n\\n      if (current._isInFirstContentOfListItem) {\\n        tokenizer._gfmTasklistFirstContentOfListItem = undefined;\\n      }\\n    } // Unravel the next token.\\n\\n\\n    previous = current;\\n    current = current.next;\\n  } // Now, loop back through all events (and linked tokens), to figure out which\\n  // parts belong where.\\n\\n\\n  current = token;\\n\\n  while (++index < childEvents.length) {\\n    if ( // Find a void token that includes a break.\\n    childEvents[index][0] === 'exit' && childEvents[index - 1][0] === 'enter' && childEvents[index][1].type === childEvents[index - 1][1].type && childEvents[index][1].start.line !== childEvents[index][1].end.line) {\\n      start = index + 1;\\n      breaks.push(start); // Help GC.\\n\\n      current._tokenizer = undefined;\\n      current.previous = undefined;\\n      current = current.next;\\n    }\\n  } // Help GC.\\n\\n\\n  tokenizer.events = []; // If there\\u2019s one more token (which is the cases for lines that end in an\\n  // EOF), that\\u2019s perfect: the last point we found starts it.\\n  // If there isn\\u2019t then make sure any remaining content is added to it.\\n\\n  if (current) {\\n    // Help GC.\\n    current._tokenizer = undefined;\\n    current.previous = undefined;\\n  } else {\\n    breaks.pop();\\n  } // Now splice the events from the subtokenizer into the current events,\\n  // moving back to front so that splice indices aren\\u2019t affected.\\n\\n\\n  index = breaks.length;\\n\\n  while (index--) {\\n    const slice = childEvents.slice(breaks[index], breaks[index + 1]);\\n    const start = startPositions.pop();\\n    jumps.unshift([start, start + slice.length - 1]);\\n    (0, _micromarkUtilChunked.splice)(events, start, 2, slice);\\n  }\\n\\n  index = -1;\\n\\n  while (++index < jumps.length) {\\n    gaps[adjust + jumps[index][0]] = adjust + jumps[index][1];\\n    adjust += jumps[index][1] - jumps[index][0] - 1;\\n  }\\n\\n  return gaps;\\n}\\n};\"],\n\"names\":[\"shadow$provide\",\"global\",\"require\",\"module\",\"exports\",\"Object\",\"defineProperty\",\"value\",\"subtokenize\",\"events\",\"jumps\",\"index\",\"event\",\"lineIndex\",\"otherIndex\",\"otherEvent\",\"more\",\"length\",\"type\",\"subevents\",\"_tokenizer\",\"_isInFirstContentOfListItem\",\"contentType\",\"assign\",\"start\",\"previous\",\"stream\",\"token\",\"context\",\"startPosition\",\"startPositions\",\"tokenizer\",\"parser\",\"childEvents\",\"gaps\",\"current\",\"adjust\",\"breaks\",\"push\",\"sliceStream\",\"next\",\"defineSkip\",\"_gfmTasklistFirstContentOfListItem\",\"write\",\"undefined\",\"line\",\"end\",\"pop\",\"slice\",\"unshift\",\"_micromarkUtilChunked\",\"splice\",\"_container\",\"parameters\"]\n}\n"]